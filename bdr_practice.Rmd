---
title: "Bayesian DB Practice"
output: html_notebook
---


```{r}
rm(list = ls())
library(data.table)
library(foreign)
library(kernlab)
library(MASS)  # for mvrnorm
library(ggplot2)
library(gridExtra)
library(glmnet)
library(caret)
setwd('~/github/bdr/')
```

## Overview

Bayesian distribution regression (BDR) is built on a few pieces of theory:

1. Kernel mean embeddings
2. Gaussian process regression (as the prior for the kernel mean embeddings)
3. Landmark points for dimensionality reduction (advantages of this over FastFood? both are reducing the dimensionality of the feature embedding so that we can calualate the empirical mean)

This notebook builds the theory of Bayesian Distribution Regression piece by piece:

1. Explore kernels from the `kernlab` package.  We'll later use the functions from this package to calculate mean embeddings for bags of observations $\{x_j^i\}_{j=1}^{N_i}$.
2. 


## Explore Kernels from `kernlab` package

```{r}
## create a RBF kernel function with sigma hyper-parameter 0.05
rbf = rbfdot(sigma = 1)

## create artificial data set
x <- matrix(rnorm(60), 6, 10)

## compute kernel matrix
kx <- kernelMatrix(rbf, x)  ## k_12 is equivalent to exp(- sum((x[1,] - x[2,])^2))
```


## Basic Gaussian process

A Gaussian process is a collection of random variables, any finite subset of which follows a multivariate normal distribution

*prior*: $f \sim \mathcal{GP}(\mathbf{0}, k(x, x'))$

We fix a finite set of $d$ points $\mathbf{s} = (s_1, \dots, s_d)$ in order to draw from the GP as a multivariate normal. Conditioning on $s$, this becomes $\mathbf{f} \sim \mathcal{N}(\mathbf{0}_d, \Sigma_d)$ where $\Sigma_d$ is the empirical covariance matrix of our points $s$.

We then ``observe" 5 points $\{x_i, y_i\}_{i = 1}^n$.  We assume that y is observed without noise such that $y = f(x)$.  In order to predict at a new point $x_\star$, we condiditon on the observed points $X$ and their associated outcomes $Y$, and arrive at the closed-form posterior predictive distribution of $f_\star$

*posterior*: $f_\star | x_\star, X, \mathbf{y} \sim \mathcal{N}(k_\star K^{-1}\mathbf{y}, k_{\star\star} - k_\star K^{-1}k_\star^T)$

where $K_{ij} = k(x_i, x_j)$ and $k_\star = [k(x_1, x_\star), \dots, k(x_n, x_\star)]$ and $k_{\star\star} = k(x_star, x_star)$

```{r}
# define function to calculate empirical covariance matrix based on Gaussian kernel with length-scale l
calcSigma = function(x1, x2, l = 1){
  Sigma = matrix(nrow = length(x1), ncol = length(x2))
  
  for(i in 1:length(x1)){
    for(j in 1:length(x2)){
      Sigma[i,j] = exp(-1/2 * ((x1[i] - x2[j])/l)^2)
    }
  }
  return(Sigma)
}

# define the points that we want to fix for the function draw
s = seq(-5, 5, by = 0.1)

# calculate the empirical covariance of the points (basically 0)
Sigma = calcSigma(s, s)

# draw 5 samples from the GP prior at the fixed points
x_prior = t(mvrnorm(n = 5, mu = rep(0, length(s)), Sigma = Sigma))
x_draws = data.frame(cbind(s, x_prior))
x_draws = melt(x_draws, id = 's', value.name = 'f(x)')

# plot draws from the prior
plot_gp_prior = ggplot(x_draws, aes(x = s, y = `f(x)`, color = variable)) + geom_line() +
  ggtitle("Draws from GP Prior") + theme_minimal()

# specify points that we observe
x_obs = c(-4, -2.5, -1, 0, 4)
y_obs = c(-2, 0, 1, 2, -1)

# calculate kernel matricies needed for posterior evaluation
K_xx = calcSigma(x_obs, x_obs)
K_xstarx = calcSigma(s, x_obs)
K_xxstar = calcSigma(x_obs, s)
K_xstarxstar = calcSigma(s, s)

# calculate posterior mean and variance
mu = K_xstarx %*% solve(K_xx) %*% y_obs
Sigma_star = K_xstarxstar - K_xstarx %*% solve(K_xx) %*% K_xxstar

# draw 5 samples from the posterior at fixed eval points s
x_post = t(mvrnorm(5, mu = mu, Sigma = Sigma_star))
x_post = data.frame(cbind(s, x_post))
x_post = melt(x_post, id = 's', value.name = 'f_star')

# calculate 95% confidence bands for posterior
f_star_covar = data.frame(cbind(s, ub = mu + 2 * sqrt(diag(Sigma_star)), lb = mu - 2 * sqrt(diag(Sigma_star))))

# plot draws from posterior
plot_gp_post = ggplot() + 
  geom_ribbon(data = f_star_covar, aes(x = s, ymin = V3, ymax = V2), fill = "grey", alpha = 0.5) +
  geom_line(data = x_post, aes(x = s, y = f_star, color = variable)) +
  #geom_point(aes(x = x_obs, y = y_obs)) +
  ggtitle("Draws from GP Posterior") +
  theme_minimal()
  
grid.arrange(plot_gp_prior, plot_gp_post, nrow = 1)
```


## Try out BDR on survey data

*Goal*: Estimate individual-level support using BDR on state-level support outcomes 

### Data

```{r}
data_sept18 = data.table(read.spss('data/Sept18/Sept18 public.sav', to.data.frame = T), stringsAsFactors = F)
data_sept18
```

Individual-level survey responses (n = `nrow(data_sept18`) from September 2018 Pew Research survey (https://www.people-press.org/dataset/september-2018-political-survey/).



#### Recode survey responses

Recode the survey responses - combine q7 (strong) and q8 (leaners) to get full support response
```{r}
## support 
data_sept18[, .N, .(q7, q8)]
data_sept18[, qsupport := NULL]
data_sept18[q7 == "Democratic Party's candidate" | q8 == "Democratic Party's candidate", qsupport := '1-D']
data_sept18[q7 == "Republican Party's candidate" | q8 == "Republican Party's candidate", qsupport := '2-R']
data_sept18[q8 == "(VOL) Other", qsupport := '3-O']
data_sept18[is.na(qsupport), qsupport := '4-DK/R']

data_sept18[, .(.N, pct = .N/nrow(data_sept18)), qsupport][order(qsupport)]
```

# calculate percentages by state
```{r}
data_state = data_sept18[, .(y_dem = mean(qsupport == '1-D')
                             , y_rep = mean(qsupport == '2-R')
                             , y_other = mean(qsupport == '3-O')
                             , y_dkr = mean(qsupport == '4-DK/R')
                             , total_respondents = .N
                             # sample covariates
                             , age_under30 = mean(as.numeric(age) < 30)
                             , race_W = mean(racecmb == 'White')
                             ), by = sstate][order(sstate)]

head(data_state)
```

# Check relationshhip between dem vote and % white by state
```{r}
plot(y_dem ~ race_W
  , data = data_state
)
```


```{r}
data_sept18
```


```{r}
data_sept18[, .N, hh1][order(hh1)]

X = data_sept18[, .(state = as.character(sstate)
                    , sex_male = as.numeric(sex == 'Male')
                    , sex_female = as.numeric(sex == 'Female')
                    , age = as.numeric(age)
                    
                    , educ_postgrad = as.numeric(grepl("Postgraduate", educ))
                    , educ_bach = as.numeric(grepl("Four year", educ))
                    , educ_assoc = as.numeric(grepl("Two year associate|Sone college", educ))
                    , educ_highschool = as.numeric(grepl("High school graduate", educ))
                    , educ_none = as.numeric(grepl("Less than high school|High school incomplete", educ))
                    
                    , hisp = as.numeric(hisp == 'Yes')
                    , race_white = as.numeric(racecmb == 'White')
                    , race_black = as.numeric(racecmb == 'Black or African-American')
                    , race_asian = as.numeric(racecmb == 'Asian or Asian-American')
                    , race_mixedother = as.numeric(racecmb == 'Mixed Race' | racecmb == 'Or some other race')
                    
                    , relig_protestant = as.numeric(grepl("Protestant|Christian", relig))
                    , relig_catholic = as.numeric(grepl("Catholic", relig))
                    , relig_athiest = as.numeric(grepl("Athiest|Agnostic", relig))
                    , relig_jewish = as.numeric(grepl("Jewish", relig))
                    , relig_muslim = as.numeric(grepl("Muslim", relig))
                    , relig_LDS = as.numeric(grepl("Mormon", relig))
                    
                    , hh_n = ifelse(hh1 == '8 or more', 8, ifelse(hh1 == "Don't know/Refused", 2, as.numeric(hh1)))
                    
                    , income_under10 = as.numeric(income == 'Less than $10,000')
                    , income_10to20 = as.numeric(income == '10 to under $20,000')
                    , income_20to30 = as.numeric(income == '20 to under $30,000')
                    , income_30to40 = as.numeric(income == '30 to under $40,000')
                    , income_40to50 = as.numeric(income == '40 to under $50,000')
                    , income_50to75 = as.numeric(income == '50 to under $75,000')
                    , income_75to100 = as.numeric(income == '75 to under $100,000')
                    , income_100to150 = as.numeric(income == '100 to under $150,000')
                    , income_over150 = as.numeric(income == 'Over $150,000')
                    , income_refused = as.numeric(income == "(VOL) Don't know/Refused")
                    )]

# center and scale X
X_numeric = scale(X[, 2:ncol(X)])

# replace NA and NaN with 0
X_numeric = apply(X_numeric, 2, function(x) {
  x[is.na(x) | is.nan(x)] <- 0
  x
})
# create full covar matrix
X = data.frame(state = X$state, X_numeric)


##### Create test/training split
set.seed(123)
# train_ind = createDataPartition(X
#                                 , y = X$state
#                                 , p = .6, list = FALSE)

train_ind = sample.int(nrow(X), size = 1000, replace = F)

X_train = X[train_ind, ]
X_test = X[-train_ind, ]

Y_train = data_sept18[train_ind, .(state = as.character(state), y_dem = as.numeric(qsupport == '1-D'))]
Y_test = data_sept18[-train_ind, .(state = as.character(state), y_dem = as.numeric(qsupport == '1-D'))]

# create data that we actually observe
Y_train_agg = Y_train[, .(y_dem_pct = mean(y_dem), .N), by = state]

Y_train_agg
```

## Mean embeddings

We observe bags of points $\{x_j^i\}_{j = 1}^{N_i}$ from $i = 1, \dots, n$, where $x_j^i \in \mathbb{R}^p$.  We have to embed the $x_j^i$ in feature space and then take the mean.  This means we can't use the kernel trick which would take us directly from $\mathbb{R}^p \times \mathbb{R}^p \to \mathbb{R}$.  In order to calculate the coordinate of each observation in feature space, we convert each point to an *explicit* featurization:  $x_j^i \in \mathbb{R}$ is mapped to $$\phi(x_j^i) = [k(x_j^i, u_1), \dots, k(x_j^i, u_d)]^T \in \mathbb{R}^d$$
where $\mathbf{u} = \{u_l\}_{l = 1}^d$ are a set of landmark points.  For now we'll set $d = 100$ and choose $u_l$ using k-means clustering.


Specify landmark points $\mathbf{u} = (u_1, \dots, u_k)$ as the centroids of k-means clustering

```{r}
n_centers = 50
groups = kmeans(X_train[, -1], centers = n_centers)
u = as.matrix(groups$centers)
head(u)
```


Create RBF kernel and get embeddings of training set $\phi(x)$ using kernel and landmark points
$$\phi(x_i) = [k(x_i, u_1), \dots, k(x_i, u_k)]$$

Choose scale parameter $\sigma$ for RBF kernel - use median heuristic for now
```{r}
rbf1 = rbfdot(sigma = 1)

K_sigma = kernelMatrix(rbf1, x = as.matrix(X_train[, -1]), y = u)
dim(K_sigma)
sigma = median(K_sigma)
```


```{r}
rbf = rbfdot(sigma = 0.1)
phi_x = kernelMatrix(rbf, x = as.matrix(X_train[, -1]), y = u)
phi_x = data.table(state = X_train$state, phi_x)
setnames(phi_x, c('state', paste0('u', 1:n_centers)))

head(phi_x)
```

Calculate mean embeddings for each state

```{r}
mu_hat = phi_x[, lapply(.SD, mean), .SDcols = names(phi_x)[2:ncol(phi_x)], by = state][order(state)]
mu_hat
```

### Model 1 - Frequentist
Normal LASSO regression on the kernel mean embeddings - no Bayesian treatment

```{r}
# fit initial lasso to find significant covars
fit_lambda = cv.glmnet(x = as.matrix(mu_hat[, -1, with = F])
                       , y = Y_train_agg[order(state), ]$y_dem_pct
                       , nfolds = 10)

# get indicies of non-0 covars (except intercept)
ind = which(coef(fit_lambda, s = 'lambda.min')[-1] != 0)

# re-fit LASSO only with non-0 covars
fit = glmnet(x = as.matrix(mu_hat[, ind + 1, with = F])
             , y = Y_train_agg[order(state), ]$y_dem_pct)
```


 Predict at the individual level
 
```{r}
# get embeddings of test points
phi_x_test = kernelMatrix(rbf, x = as.matrix(X_test[, -1]), y = u)
phi_x_test = data.table(state = X_test$state, phi_x_test)
setnames(phi_x_test, c('state', paste0('u', 1:n_centers)))

y_hat = predict(fit, newx = as.matrix(X_test[, ind + 1]), s= 0)
y_hat = cbind(mu_hat_gender[,.(state,sex)], y_hat = y_hat)

y_actual = data_sept18[, .(y_dem = mean(as.numeric(qsupport == '1-D'))), by = .(sstate,sex)]
setnames(y_actual, c('state', 'sex', 'y_dem'))
y_hat = merge(y_actual, y_hat, by = c('state','sex'), all = T)

plot(y_hat$y_hat.1 ~ y_hat$y_dem, col = y_hat$sex)
```




## THINGS TO DO

*parameter choice*:

- number of landmark points (how is this normally chosen?)
- Calculate landmark points with full data set or just training?
- scale parameter $\sigma$ for RBF kernel (right now just usiing median heuristic)

Bags based on demos instead of state (more specific to this poll than most we'd be dealing with)?

*Choose data set*:
* census microdata with actual county-level election outcomes. pro: large (like actual voterfile), con: outcomes not at the individual-level
* Pew survey data. pro: outcomes at the individual-level, con: it's small
* census microdata for covar data with survey data for outomes.  Pro: large covar data, closer to real scenario, con: can only link at the state-level, still no individual-level outcome data
* synthetic data. pro: can make it exactly what we want, con: effort to synthesize

** benefit of the larger data set - more realistic, we'd really need the landmark points when we don't if only working with the survey data

Compare against lasso/custom-built logit or maybe NN that has access to the individual-level outcomes (more realistic benchmark than)

**Implement full Bayesian treatment (uncertainty in mean embeddings as well as bag size)

MRP - standard for EI in political applications









